{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df252c16-1ce8-4bd4-a373-6d6eb1e5f653",
   "metadata": {},
   "source": [
    "## Execice 2 : Utilisation de transformers pour les deux datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a205a5c-bcfe-4492-8d8b-b725e7c416bd",
   "metadata": {},
   "source": [
    "### 2.1 :  Utilisation d'un transformer prêt à l'emploi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d85f42eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a173237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les datasets\n",
    "class FakeNewsData(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_and_prepare_datasets(dataframe, text_field, target_field, tokenizer, max_seq_length):\n",
    "    texts_train, texts_test, labels_train, labels_test = train_test_split(\n",
    "        dataframe[text_field].values,\n",
    "        dataframe[target_field].values, \n",
    "        test_size=0.2,\n",
    "        random_state=42 \n",
    "    )\n",
    "    train_data = FakeNewsData(texts_train, labels_train, tokenizer, max_seq_length)\n",
    "    test_data = FakeNewsData(texts_test, labels_test, tokenizer, max_seq_length)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46658c82-2760-4805-a710-01cedd8c8236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset en anglais :\n",
      "                                               title  \\\n",
      "0  Three more states refuse Trump commission's vo...   \n",
      "1   Trump Crosses The Line, Attacks Civil Rights ...   \n",
      "2  IS TRUMP A RACIST? Famous Italian-American “Ge...   \n",
      "3   Ted Cruz Gets His Unethical A** Handed To Him...   \n",
      "4  Putin says Trump hampered from delivering elec...   \n",
      "\n",
      "                                                text  label  \n",
      "0  WASHINGTON (Reuters) - Maryland, Delaware and ...      1  \n",
      "1  Georgia Congressman John Lewis is one of Ameri...      0  \n",
      "2  Robert Davi gives a great answer to Neil Cavut...      0  \n",
      "3  Seth Meyers destroyed Republican presidential ...      0  \n",
      "4  SOCHI, Russia (Reuters) - Russian President Vl...      1  \n",
      "Index(['title', 'text', 'label'], dtype='object')\n",
      "\n",
      "Dataset en français :\n",
      "                                               title  \\\n",
      "0  Les chefs de service hospitaliers en appellent...   \n",
      "1  L'origine des comportements alimentaires ident...   \n",
      "2  Microsoft alerte sur de nouvelles techniques d...   \n",
      "3  L'Europe se dote d'une infrastructure de téléc...   \n",
      "4  Science décalée : la créature la plus rapide s...   \n",
      "\n",
      "                                         description  \\\n",
      "0  Jugeant très insuffisante la réponse du gouver...   \n",
      "1           D'après une étude publiée dans la revue    \n",
      "2  Le géant de l'informatique Microsoft vient de ...   \n",
      "3  Pour garantir la sécurité de la libre circulat...   \n",
      "4  Le guépard est l'animal le plus rapide... sur ...   \n",
      "\n",
      "                                                text  fake  \n",
      "0  Les chefs de service hospitaliers en appellent...     0  \n",
      "1  L'origine des comportements alimentaires ident...     0  \n",
      "2  Microsoft alerte sur de nouvelles techniques d...     0  \n",
      "3  L'Europe se dote d'une infrastructure de téléc...     0  \n",
      "4  Science décalée : la créature la plus rapide s...     0  \n",
      "Index(['title', 'description', 'text', 'fake'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_en = pd.read_csv(\"data_fake_news_en.csv\")\n",
    "\n",
    "df_fr = pd.read_csv(\"fake_news_fr.csv\")\n",
    "\n",
    "# Afficher les premières lignes pour vérifier la structure\n",
    "print(\"Dataset en anglais :\")\n",
    "print(df_en.head())\n",
    "print(df_en.columns)\n",
    "\n",
    "print(\"\\nDataset en français :\")\n",
    "print(df_fr.head())\n",
    "print(df_fr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b63bcbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36335    Democrats want to spend a whopping $2 billion ...\n",
      "12384    TUNIS (Reuters) - Tunisia will continue with a...\n",
      "24419    Wouldn t it be great if we had someone in gove...\n",
      "24740     Last week, President Trump made a public anno...\n",
      "27039    WASHINGTON (Reuters) - The United States, Cana...\n",
      "                               ...                        \n",
      "11284    Michelle Obama fed her husband s feud with Don...\n",
      "44732    WASHINGTON (Reuters) - Republican Mitt Romney ...\n",
      "38158    Climate grifter Al Gore is confronted about gl...\n",
      "860      It s gotten to the point that if you re still ...\n",
      "15795    WASHINGTON (Reuters) - The U.S. Supreme Court ...\n",
      "Name: text, Length: 35918, dtype: object\n",
      "2365    « L’Ukraine est à genoux et tout le monde s’en...\n",
      "1100    Les 5 infos dans le rétro du week-end : Tensio...\n",
      "1526    Un gagnant de l’Euromillion flambe et dilapide...\n",
      "298     Dossier : balade dans le curieux monde des fra...\n",
      "927     Municipales 2020 au Mans : Stéphane Le Foll dé...\n",
      "                              ...                        \n",
      "1638    #Débathon sur Twitch : Édouard Philippe promet...\n",
      "1095    Plus précis mais interdit. Un fabricant de LBD...\n",
      "1130    Grève à la SNCF : Le taux de grévistes tombe à...\n",
      "1294    Comment incarner un père Noël crédible sans se...\n",
      "860     Retraites : Edouard Philippe convie les parten...\n",
      "Name: text, Length: 1945, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diviser le dataset en anglais\n",
    "X_train_en, X_test_en, y_train_en, y_test_en = train_test_split(\n",
    "    df_en['text'], df_en['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "print(X_train_en)\n",
    "# Diviser le dataset en français\n",
    "X_train_fr, X_test_fr, y_train_fr, y_test_fr = train_test_split(\n",
    "    df_fr['text'], df_fr['fake'], test_size=0.2, random_state=42\n",
    ")\n",
    "print(X_train_fr)\n",
    "\n",
    "X_test_sample_en = X_test_en.sample(n=100, random_state=42)\n",
    "y_test_sample_en = y_test_en.loc[X_test_sample_en.index].tolist()\n",
    "\n",
    "X_test_sample_fr = X_test_fr.sample(n=100, random_state=42)\n",
    "y_test_sample_fr = y_test_fr.loc[X_test_sample_fr.index].tolist()\n",
    "\n",
    "\n",
    "X_test_sample_en = X_test_sample_en.tolist()\n",
    "X_test_sample_fr = X_test_sample_fr.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc66eb7-88bf-4355-8ae8-f1b293696933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport de classification (anglais) :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.82      0.60        45\n",
      "           1       0.62      0.24      0.34        55\n",
      "\n",
      "    accuracy                           0.50       100\n",
      "   macro avg       0.54      0.53      0.47       100\n",
      "weighted avg       0.55      0.50      0.46       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifier_en = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "X_test_sample_en = [text[:512] for text in X_test_sample_en]\n",
    "y_pred_en = [classifier_en(text)[0]['label'] for text in X_test_sample_en]\n",
    "y_pred_en = [1 if label == \"POSITIVE\" else 0 for label in y_pred_en]\n",
    "print(\"Rapport de classification (anglais) :\")\n",
    "print(classification_report(y_test_sample_en, y_pred_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af16c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport de classification (français) :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.80        66\n",
      "           1       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.66       100\n",
      "   macro avg       0.33      0.50      0.40       100\n",
      "weighted avg       0.44      0.66      0.52       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khadijaakkar/Documents/3A/RI/TP/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/khadijaakkar/Documents/3A/RI/TP/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/khadijaakkar/Documents/3A/RI/TP/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifier_fr = pipeline(\"text-classification\", model=\"camembert-base\")\n",
    "X_test_sample_fr = [text[:512] for text in X_test_sample_fr]\n",
    "y_pred_fr = [classifier_fr(text)[0]['label'] for text in X_test_sample_fr]\n",
    "y_pred_fr = [1 if label == \"POSITIVE\" else 0 for label in y_pred_fr]\n",
    "print(\"Rapport de classification (français) :\")\n",
    "print(classification_report(y_test_sample_fr, y_pred_fr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b423099-c5fc-436b-9a66-ec54e7d3322a",
   "metadata": {},
   "source": [
    "### 2.2 Finetuning d'un modèle de transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "31762455",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_englais = \"distilbert-base-uncased\"\n",
    "model_français = \"camembert-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe49cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_englais = AutoTokenizer.from_pretrained(model_englais)\n",
    "train_englais, test_englais = split_and_prepare_datasets(df_en, \"text\", \"label\", tokenizer_englais, max_length)\n",
    "tokenizer_français = AutoTokenizer.from_pretrained(model_français)\n",
    "train_français, test_français = split_and_prepare_datasets(df_fr, \"text\", \"fake\", tokenizer_français, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "242dee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "import torch\n",
    "\n",
    "def fine_tune_transformer(model, training_loader, validation_loader, num_epochs, device):\n",
    "    model_optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() \n",
    "        epoch_loss, correct_predictions = 0, 0\n",
    "        for data_batch in training_loader:\n",
    "            input_ids = data_batch[\"input_ids\"].to(device)\n",
    "            attention_masks = data_batch[\"attention_mask\"].to(device)\n",
    "            target_labels = data_batch[\"label\"].to(device)\n",
    "            model_optimizer.zero_grad()\n",
    "            model_outputs = model(input_ids, attention_mask=attention_masks)\n",
    "            batch_loss = criterion(model_outputs.logits, target_labels)\n",
    "            batch_loss.backward()\n",
    "            model_optimizer.step()\n",
    "            epoch_loss += batch_loss.item()\n",
    "            correct_predictions += (model_outputs.logits.argmax(1) == target_labels).sum().item()\n",
    "        avg_loss = epoch_loss / len(training_loader)\n",
    "        accuracy = correct_predictions / len(training_loader.dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ed7b0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Finetuning_model(train_dataset, test_dataset, model_name, epochs, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "    \n",
    "    model = fine_tune_transformer(model, train_loader, test_loader, epochs, device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bf555",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finetuning_model_français = Finetuning_model(train_français, test_français, model_français, epochs=3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1812a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finetuning_model_englais = Finetuning_model(train_englais, test_englais, model_englais, epochs=3, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
